{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "\n",
    "![](img/Datasyn_Ø1-task1a.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "![](img/Datasyn_Ø1-task1b-1.jpg)\n",
    "![](img/Datasyn_Ø1-task1b-2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "source": [
    "## Task 2a)\n",
    "\n",
    "For naormalizing the images i chose a linear function, that converts numbers from \\[0,255\\] -> \\[-1,1\\]. Now it is also quite easy to apply a sigmaoid function if it can give better result."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "![](img/task2b_binary_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](img/task2b_binary_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "\n",
    "The trainings stops after 33 epochs. The end accuracy results are also better then previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "\n",
    "![](img\\task2e_train_loss_with_shuffle.png)\n",
    "![](img\\task2e_train_accuracy_shuffle_difference.png)\n",
    "\n",
    "We can see that data shuffling removes many of the spikes that we could see prevoiusly. They were result of some difficult batch. When wwe shuffel the data the \"difficult\" images are spread around and the spikes disapear. We can also see that early stopping kicks in a bit earlier (after 24 batches). It will however vary from time to time since the data is shuffled randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "![](img\\task3b_softmax_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "![](img\\task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "The first sign of overfitting is that the training loss is not improving very much afterabot 3000 samples. We can also see that the validation accuracy is getting quite lower than the training accuracy (also after about 3000 samples). It means that the network has learned the traingin set good, but is worsa at recognising new data. Which is overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "![](img\\Datasyn_Ø1-task4a.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "\n",
    "lambda = 0.0\n",
    "![](img\\task4b_softmax_weight_l_0.png)\n",
    "\n",
    "lambda = 1.0\n",
    "![](img\\task4b_softmax_weight_l_1.png)\n",
    "\n",
    "With lambda=1 the gradiant will be smaller and weights wil also change slower and be smaller. Smaller weights leads to less complex models that doesn't overfit that much. We can often see that overfitting model will take the noise into account and becomes less general. The L2 regression makes makes model more general and we can see that clearly from the images of the weights. The weight without regularization are much morenoisy then those with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "\n",
    "![](img\\task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "\n",
    "With regularization the model might become too \"simple\" for the task and wont be able respond well for more varying inputs. This may not be a problem when working with more complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "\n",
    "![](img\\task4d_l2_reg_norms.png)\n",
    "\n",
    "We can see that the higher the lambda is, the smaller the weights get. The smaller weights will lead to simpler model and less overfitting. This confirms what we see both in task b and c."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python38164bitpy38condac1f68ca5407a4349b0d7e37676f2fbb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}