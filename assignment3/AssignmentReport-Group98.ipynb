{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "![](images/1a.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "max-polling\n",
    "\n",
    "## task 1c)\n",
    "\n",
    "padding = 1\n",
    "\n",
    "## task 1defg)\n",
    "\n",
    "![](images/1def.jpg)\n",
    "![](images/1g.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "\n",
    "My idea here was to make one network that uses more cnvolutional layer and few fully connected layers, in this model tweaking the architecture should be enough to achieve 75% accuracy. The other network should have more complex fully connected layer, and maybe fewer convolution layers, so I could use other methods to improve the performence.\n",
    "\n",
    "Model1\n",
    "\n",
    "\n",
    "| Layer | Layer Type      | Hidden units/Filters | Activation function |\n",
    "|-------|-----------------|----------------------|---------------------|\n",
    "| 1     | Conv2d          | 128                  | ReLU                |\n",
    "| 2     | Conv2d          | 256                  | ReLU                |\n",
    "| 2     | MaxPool2d       |                      |                     |\n",
    "| 3     | Conv2d          | 512                  | ReLU                |\n",
    "| 4     | Conv2d          | 512                  | ReLU                |\n",
    "| 4     | MaxPool2d       |                      |                     |\n",
    "|-------|-----------------|----------------------|---------------------|\n",
    "| -     | Flatten         |                      |                     |\n",
    "| 5     | Linear          | 64                   | ReLU                |\n",
    "| 6     | Linear          | 64                   | ReLU                |\n",
    "| 6     | Linear          | 10                   | Softmax             |\n",
    "\n",
    "\n",
    "* Data augmentation: RandomRotation, RandomGrayscale\n",
    "* Optimizer: SGD, no momentum, no Regularization\n",
    "* Batch size: 64\n",
    "* Learning rate: 5e-2\n",
    "\n",
    "\n",
    "\n",
    "Model2\n",
    "\n",
    "| Layer | Layer Type      | Hidden units/Filters | Activation function |\n",
    "|-------|-----------------|----------------------|---------------------|\n",
    "| 1     | Conv2d          | 150                  | ReLU                |\n",
    "| 1     | MaxPool2d       |                      |                     |\n",
    "| 1     | BatchNorm2d     |                      |                     |\n",
    "| 2     | Conv2d          | 256                  | ReLU                |\n",
    "| 2     | MaxPool2d       |                      |                     |\n",
    "| 2     | BatchNorm2d     |                      |                     |\n",
    "|-------|-----------------|----------------------|---------------------|\n",
    "| -     | Flatten         |                      |                     |\n",
    "| -     | BatchNorm1d     |                      |                     |\n",
    "| 5     | Linear          | 128                  | ReLU                |\n",
    "| 5     | Dropout         | p=0.1                |                     |\n",
    "| 6     | Linear          | 128                  | ReLU                |\n",
    "| 6     | Dropout         | p=0.1                |                     |\n",
    "| 7     | Linear          | 32                   | ReLU                |\n",
    "| 8     | Linear          | 10                   | Softmax             |\n",
    "\n",
    "\n",
    "* Data augmentation: None\n",
    "* Optimizer: Adagrad\n",
    "* Batch size: 64\n",
    "* Learning rate: 0.001 (standard adagard lr)\n",
    "\n",
    "\n",
    "### Task 3b)\n",
    "\n",
    "Both of my models achieved very similiar final performance so It's hard to decide which one is best. It was however much easier to improve model1.\n",
    "\n",
    "|      -      | model 1         | model2          |\n",
    "|-------------|-----------------|-----------------|\n",
    "| train loss  | 0.35            | 0.35            |\n",
    "| train acc.  | 0.88            | 0.88            |\n",
    "| val. acc.   | 0.76            | 0.76            |\n",
    "| test acc.   | 0.78            | 0.75            |\n",
    "\n",
    "\n",
    "\n",
    "#### Model1\n",
    "\n",
    "* Train loss: 0.35, Train accuracy: 0.88\n",
    "* Validation loss: 0.69, Validation accuracy: 0.76\n",
    "* Test loss: 0.67, Test accuracy: 0.78\n",
    "\n",
    "![](plots/task3_model1_plot.png)\n",
    "\n",
    "\n",
    "#### Model 2\n",
    "\n",
    "* Train loss: 0.35, Train accuracy: 0.88\n",
    "* Validation loss: 0.74, Validation accuracy: 0.76\n",
    "* Test loss: 0.77, Test accuracy: 0.75\n",
    "\n",
    "![](plots/task3_model2_plot.png)\n",
    "\n",
    "### Task 3c)\n",
    "* Incresing number of convolution layers generally improved the performance, icreasing complexity of the fully connected layers had little improvement. This means that fetuyre extraction is the part that does tha main work in the image recognition. However after some point the performance didnt get any better after adding more filters and convolution layer. This might be because the model was complex enough for the problem.\n",
    "* Since we can not maxpool images infinitly beacuse the size will get too small, I decided to keep two maxPool layers. The network gave best results when th maxPool layers was spread out in the layer (for instance with two conv layers in between).\n",
    "* Batch normalization improved both the end accuracy and reduces the training time needed. It's an expected effect and it had great effect on the performance. \n",
    "* Adding momentum had small improvement, adding L2 weight normalization made the performance worse. Here it would be neccesary to tune the hyperparameters. It was much easier and more effective to add dropout to eliminate some overfitting.\n",
    "* By making the network more complex, we might need to update the learning rate. It can be quite difficult to find optimal learning rate when using SGD, the quite easy solution was to change to an adaptive learning rate method. The method that gave best results for my network was Adagrad, and improved the performance specially when the Dropouts were added.\n",
    "* Dropout on fully conncted layer had quite a big impact on reducing the overfitting. With dropout the validation loss and training loss were almost identical. It increased however the learning time, which is to be expected after adding extra operations.\n",
    "* ReLU has been much better activation function compared to tanh and sigmaoid. ReLU  minimzes problem with small derivatives far away from 0, which we saw was a problem in assignment 1 and 2. Other functions like ELU and LekingReLU had very similar or slightly worse performance then ReLU.\n",
    "* Reducing the filter size from 5 to 3 had a little improvement in the performance. Small filter might be better at findig features in smaller images like ours.\n",
    "* Adding data augmentation resulted in better test accuracy even tho the final validation and training accuracy decreased. Reason for this might be that both validation data and training data got augmented (ideally only training data should be augmented), but it resulted in more general model that performes better on unseen data.\n",
    "* Decreasing batch size had little effect on the accuracy, but incresed the runtime. Incresing the batch size signinficantly had negative effect on the final accuracy. \n",
    "\n",
    "### Task 3d)\n",
    "\n",
    "Here we can see comparrasion of network similiar to model2 from previous task with and without batch normalization. As we can see the training is much faster and gives better results.  \n",
    "\n",
    "![](plots/task3d_plot.png)\n",
    "\n",
    "\n",
    "### Task 3e)\n",
    "\n",
    "| Layer | Layer Type      | Hidden units/Filters | Activation function |\n",
    "|-------|-----------------|----------------------|---------------------|\n",
    "| 1     | Conv2d          | 128                  | ReLU                |\n",
    "| 1     | BatchNorm2d     |                      |                     |\n",
    "| 2     | Conv2d          | 128                  | ReLU                |\n",
    "| 2     | BatchNorm2d     |                      |                     |\n",
    "| 2     | MaxPool2d       |                      |                     |\n",
    "| 3     | Conv2d          | 128                  | ReLU                |\n",
    "| 3     | BatchNorm2d     |                      |                     |\n",
    "| 4     | Conv2d          | 128                  | ReLU                |\n",
    "| 4     | BatchNorm2d     |                      |                     |\n",
    "| 4     | MaxPool2d       |                      |                     |\n",
    "| 5     | Conv2d          | 128                  | ReLU                |\n",
    "| 5     | BatchNorm2d     |                      |                     |\n",
    "| 6     | Conv2d          | 128                  | ReLU                |\n",
    "| 6     | BatchNorm2d     |                      |                     |\n",
    "|-------|-----------------|----------------------|---------------------|\n",
    "| -     | Flatten         |                      |                     |\n",
    "| -     | BatchNorm1d     |                      |                     |\n",
    "| 7     | Linear          | 256                  | ReLU                |\n",
    "| 7     | BatchNorm1d     |                      |                     |\n",
    "| 7     | Dropout         | p=0.4                |                     |\n",
    "| 8     | Linear          | 10                   | Softmax             |\n",
    "\n",
    "Final results:\n",
    "* Train loss: 0.26, Train accuracy: 0.91\n",
    "* Validation loss: 0.58, Validation accuracy: 0.80\n",
    "* Test loss: 0.53, Test accuracy: 0.83\n",
    "\n",
    "![](plots/task3e_plot.png)\n",
    "\n",
    "### Task 3f)\n",
    "\n",
    "We can see that the final model is still overfitting a bit, since validation loss and training loss is not same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "![](plots/task4a_plot.png)\n",
    "The accuracy plot has quite small interval on y-axis whis make it seem like big variations \n",
    "\n",
    "\n",
    "Final results:\n",
    "* Train loss: 0.13, Train accuracy: 0.96\n",
    "* Validation loss: 0.33, Validation accuracy: 0.89\n",
    "* Test loss: 0.37, Test accuracy: 0.88\n",
    "\n",
    "Hyperparameters:\n",
    "* batch_size = 32\n",
    "* learning_rate = 5e-4\n",
    "* optimizer: Adam\n",
    "* data augmentation: resize and normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "\n",
    "We can clearly see that different layers extranct differen features of the image. For instance filter 3 and 5 in the figure below extracts pretty well the outline shape of the zebra, these two filter are also almost negatives of eachother. Filter 1 extracts vertical lines and filter 2 horizontal lines, whihch is easily visible on the zebra.\n",
    "\n",
    "![](images/4b.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "\n",
    "The last layers contain more low-level features, like part of a line. It's no longer possible tor recognize the image features by eye. Found feature have much higher contrast, more clear what feture has been extracted. It is to be expected when an image has been passed through a lot of convolution layers.\n",
    "\n",
    "![](images/4c.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('tdt4265': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8c6698b9213cf3fd47742b57f2be4fb1179feacd358afc460ab8195cabfe8ab4"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}